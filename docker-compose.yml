version: '3.8'

services:
  # n8n - Plataforma de automatización
  n8n:
    image: n8nio/n8n:latest
    restart: always
    ports:
      - "5678:5678"
    environment:
      - N8N_SECURE_COOKIE=false
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=PSW1234
      - N8N_HOST=10.128.0.5
      - N8N_EDITOR_BASE_URL=http://rem.extrimian.com
      - WEBHOOK_URL="http://rem.extrimian.com"
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_ENCRYPTION_KEY=PSW.192837
      - NODE_ENV=production
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      - postgres

  # Base de datos para n8n
  postgres:
    image: postgres:13
    restart: always
    environment:
      - POSTGRES_USER=n8n
      - POSTGRES_PASSWORD=PSW.192837
      - POSTGRES_DB=n8n
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Servidor LLM local (Ollama)
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    #command: ollama run mistral:7b

  # Interfaz web para tu LLM local
  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    restart: always
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434/api
      - PUBLIC_OLLAMA_API_BASE_URL=http://logan5.extrimian.com:11434/api  # Cambia esto por la IP externa de tu VM
    depends_on:
      - ollama

  # Servicio de la aplicación web Streamlit
  rag-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-app
    volumes:
      - ./documentos:/app/documentos  # Montar carpeta de documentos
      - ./faiss_index:/app/faiss_index  # Montar carpeta para el índice
    ports:
      - "8501:8501"  # Puerto de Streamlit
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia  # Usar runtime de nvidia
    depends_on:
      - ollama
    restart: unless-stopped

  # API RAG
  rag-api:
    build: 
      context: .
      dockerfile: Dockerfile.api
    container_name: rag-api
    volumes:
      - ./documentos:/app/documentos  # Montar carpeta de documentos
      - ./faiss_index:/app/faiss_index  # Montar carpeta de índice FAISS
    ports:
      - "8000:8000"  # Exponer puerto de FastAPI
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia  # Usar runtime de nvidia
    depends_on:
      - ollama
      - rag-app
    restart: unless-stopped

volumes:
  n8n_data:
  postgres_data:
  ollama_data:
